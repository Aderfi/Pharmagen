======================================================================
MODEL TRAINING REPORT
======================================================================

Model Configuration:
  Model Name: Phenotype_Effect_Outcome
  Target Columns: phenotype_outcome, effect_direction, effect_type

Training Results:
  Best Validation Loss: 2.79214

Per-Task Average Losses:
  phenotype_outcome: 0.26511
  effect_direction: 0.98965
  effect_type: 1.81053

Per-Task Best Accuracies:
  phenotype_outcome: 0.8689
  effect_direction: 0.6607
  effect_type: 0.4673

Hyperparameters:
  embedding_dim: 256
  n_layers: 2
  hidden_dim: 2176
  dropout_rate: 0.6826665445146843
  weight_decay: 0.0001478755319922093
  learning_rate: 0.0002023597603520488
  batch_size: 64
  
            # === TRANSFORMER ATTENTION (based on your attention_layer) ===
            "attention_dim_feedforward": ["int", 512, 4096, 256],  # For transformer feedforward
            "attention_dropout": (0.1, 0.5),
            "num_attention_layers": [1, 2, 3, 4],  # Stack multiple transformer layers
            
            # === FOCAL LOSS (you're using it for effect_type) ===
            "focal_gamma": (1.0, 5.0),  # Currently hardcoded to 2.0
            "focal_alpha_weight": (0.5, 3.0),  # Multiplier for class weights
            "label_smoothing": (0.05, 0.3),  # Currently hardcoded to 0.15
            
            # === OPTIMIZER VARIANTS ===
            "optimizer_type": ["adamw", "adam", "sgd", "rmsprop"],
            "adam_beta1": (0.8, 0.95),
            "adam_beta2": (0.95, 0.999),
            "sgd_momentum": (0.85, 0.99),
            
            # === SCHEDULER ===
            "scheduler_type": ["plateau", "cosine", "step", "exponential", "none"],
            "scheduler_factor": (0.1, 0.8),  # For ReduceLROnPlateau
            "scheduler_patience": ["int", 3, 15, 1],
            
            # === TASK WEIGHTING ===
            "uncertainty_weighting": [True, False],  # Toggle your uncertainty weighting
            "manual_task_weights": [True, False],  # Use CLINICAL_PRIORITIES vs learned weights
            
            # === ADVANCED ARCHITECTURE ===
            "use_batch_norm": [True, False],
            "use_layer_norm": [True, False], 
            "activation_function": ["gelu", "relu", "swish", "mish"],  # You're using GELU
            "gradient_clip_norm": (0.5, 5.0),
            
            # === FM BRANCH ENHANCEMENTS ===
            "fm_dropout": (0.1, 0.5),  # Separate dropout for FM interactions
            "fm_hidden_layers": [0, 1, 2],  # Add layers after FM interactions
            "fm_hidden_dim": ["int", 64, 512, 32],
            
            # === EMBEDDING VARIATIONS ===
            "embedding_dropout": (0.1, 0.3),
            "drug_embedding_dim": ["int", 64, 1024, 32],  # Separate embedding dims
            "gene_embedding_dim": ["int", 64, 1024, 32],
            "allele_embedding_dim": ["int", 32, 512, 16],
            "genalle_embedding_dim": ["int", 64, 1024, 32],
            
            # === TRAINING DYNAMICS ===
            "warmup_epochs": ["int", 0, 20, 1],
            "early_stopping_patience": ["int", 15, 50, 5],
            "validation_frequency": ["int", 1, 5, 1],  # Validate every N epochs
        : None
  attention_dim_feedforward: 3840
  attention_dropout: 0.48387265129374835
  num_attention_layers: 2
  focal_gamma: 1.1605241620974445
  focal_alpha_weight: 1.3417203741569734
  label_smoothing: 0.11100845590336689
  optimizer_type: rmsprop
  manual_task_weights: True
  use_batch_norm: True
  use_layer_norm: True
  activation_function: gelu
  gradient_clip_norm: 2.231503593586354
  fm_dropout: 0.34731215338736454
  fm_hidden_layers: 2
  fm_hidden_dim: 384
  embedding_dropout: 0.1546248948072961
  early_stopping_patience: 25
  scheduler_type: none

======================================================================
