Package Context
- DeepFM_PGenModel (pgen_model/src/model.py) combines shared embeddings, transformer-style attention, FM branch, and per-task heads. Attention requires embedding_dim divisible by 8, and separate per-field embedding dims must stay consistent or torch.stack will fail.
- pipeline.py orchestrates training: data import/cleaning, stratified split, soft class weights for effect_type, model creation, optimizer/loss setup, and training invocation.
- train.py implements the AMP-enabled loop (early stopping, validation metrics, Optuna integration) summing per-task losses.
- train_utils.py builds optimizers, schedulers, and loss functions; many optional flags (uncertainty_weighting, gradient_clip_norm, etc.) are not yet wired into the loop.
- optuna_train.py drives hyperparameter search using MODEL_REGISTRY params_optuna; supports multi-objective optimization and logs per-task metrics.
- data.py handles preprocessing with fit/transform, unknown-token replacement, and limited multi-label support (phenotype_outcome).

Data Signals (pgen_model/train_data/final_test_genalle.tsv)
- Rows: 13158. Unique values -> Drug: 1325, Genalle: 4442, Gene: 1487, Allele: 741.
- Targets -> Phenotype_outcome: 22 labels (â‰ˆ4.6% multi-label rows), Effect_direction: 3 labels, Effect_type: 19 labels with long-tail imbalance (top 6 classes >1000 samples, tail <100).
- Phenotype_outcome label counts: mean 1.05 per row, median 1, max 4, 603 rows with >1 label.

Suggested params_optuna Ranges
- embedding_dim: categorical {128,160,192,224,256,288,320,352,384,416,448} (ensure divisible by 8).
- n_layers: integer [2,5].
- hidden_dim: integer uniform 512-2048 step 256.
- attention_dim_feedforward: integer choices {hidden_dim, hidden_dim*2}; ensure >= hidden_dim.
- dropout_rate: float 0.10-0.45.
- embedding_dropout: float 0.00-0.20.
- attention_dropout: float 0.00-0.30.
- fm_dropout: float 0.00-0.25.
- label_smoothing: float 0.05-0.18.
- focal_gamma: float 1.5-3.5.
- focal_alpha_weight: float 0.8-1.8.
- batch_size: categorical {64,96,128,160,192,224,256}.
- learning_rate: float log-uniform 1e-5 to 3e-4.
- weight_decay: float log-uniform 1e-6 to 3e-4.
- optimizer_type: categorical {"adamw","adam"} (extend only if extra logic added).
- adam_beta1: float 0.85-0.95.
- adam_beta2: float 0.96-0.999.
- scheduler_type: categorical {"plateau","cosine","none"}.
- scheduler_factor: float 0.20-0.60.
- scheduler_patience: integer [3,8].
- epochs: integer [60,140].
- patience: integer [10,25].
- fm_hidden_layers: categorical {0,1,2}.
- fm_hidden_dim: integer 128-448 step 64.

Omit or postpone in params_optuna
- Separate per-field embedding dims unless you adjust model.forward stacking logic.
- Flags like uncertainty_weighting, manual_task_weights, gradient_clip_norm, warmup_epochs, validation_frequency until they are connected in training code.

Next Steps
1. Update MODEL_REGISTRY entry for Phenotype_Effect_Outcome with the ranges above.
2. Remove unused options from params_optuna to keep Optuna search coherent.
3. Run a handful of dry Optuna trials to confirm sampled embeddings satisfy attention head divisibility and loss weighting stays valid.
