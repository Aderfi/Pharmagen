Modelo: Phenotype_Effect_Outcome_Kv
Mejor loss en validación: -3.53428
Precisión Promedio (Avg Acc) en mejor época: 0.6603
Precisión por Tarea:
  - phenotype_outcome: 0.8809
  - effect_direction: 0.6702
  - effect_type: 0.4297
Pérdida Promedio por Tarea (última época):
  - phenotype_outcome: 0.3149
  - effect_direction: 1.1499
  - effect_type: 0.3491

Hiperparámetros Utilizados:
  embedding_dim: 128
  n_layers: 5
  hidden_dim: 3072
  dropout_rate: 0.39068520160372305
  weight_decay: 5.0317738624675386e-05
  learning_rate: 0.0005730468431792992
  batch_size: 64
  
            # === TRANSFORMER ATTENTION (based on your attention_layer) ===
            "attention_dim_feedforward": ["int", 512, 4096, 256],  # For transformer feedforward
            "attention_dropout": (0.1, 0.5),
            "num_attention_layers": [1, 2, 3, 4],  # Stack multiple transformer layers
            
            # === FOCAL LOSS (you're using it for effect_type) ===
            "focal_gamma": (1.0, 5.0),  # Currently hardcoded to 2.0
            "focal_alpha_weight": (0.5, 3.0),  # Multiplier for class weights
            "label_smoothing": (0.05, 0.3),  # Currently hardcoded to 0.15
            
            # === OPTIMIZER VARIANTS ===
            "optimizer_type": ["adamw", "adam", "sgd", "rmsprop"],
            "adam_beta1": (0.8, 0.95),
            "adam_beta2": (0.95, 0.999),
            "sgd_momentum": (0.85, 0.99),
            
            # === SCHEDULER ===
            "scheduler_type": ["plateau", "cosine", "step", "exponential", "none"],
            "scheduler_factor": (0.1, 0.8),  # For ReduceLROnPlateau
            "scheduler_patience": ["int", 3, 15, 1],
            
            # === TASK WEIGHTING ===
            "uncertainty_weighting": [True, False],  # Toggle your uncertainty weighting
            "manual_task_weights": [True, False],  # Use CLINICAL_PRIORITIES vs learned weights
            
            # === ADVANCED ARCHITECTURE ===
            "use_batch_norm": [True, False],
            "use_layer_norm": [True, False], 
            "activation_function": ["gelu", "relu", "swish", "mish"],  # You're using GELU
            "gradient_clip_norm": (0.5, 5.0),
            
            # === FM BRANCH ENHANCEMENTS ===
            "fm_dropout": (0.1, 0.5),  # Separate dropout for FM interactions
            "fm_hidden_layers": [0, 1, 2],  # Add layers after FM interactions
            "fm_hidden_dim": ["int", 64, 512, 32],
            
            # === EMBEDDING VARIATIONS ===
            "embedding_dropout": (0.1, 0.3),
            "drug_embedding_dim": ["int", 64, 1024, 32],  # Separate embedding dims
            "gene_embedding_dim": ["int", 64, 1024, 32],
            "allele_embedding_dim": ["int", 32, 512, 16],
            "genalle_embedding_dim": ["int", 64, 1024, 32],
            
            # === TRAINING DYNAMICS ===
            "warmup_epochs": ["int", 0, 20, 1],
            "early_stopping_patience": ["int", 15, 50, 5],
            "validation_frequency": ["int", 1, 5, 1],  # Validate every N epochs
        : None
  attention_dim_feedforward: 1024
  attention_dropout: 0.16425453485782834
  num_attention_layers: 1
  focal_gamma: 2.8837712550980745
  focal_alpha_weight: 0.3379788567248563
  label_smoothing: 0.007241126339333478
  optimizer_type: rmsprop
  manual_task_weights: True
  use_batch_norm: True
  use_layer_norm: True
  activation_function: relu
  gradient_clip_norm: 3.5881973750267306
  fm_dropout: 0.4495053652272731
  fm_hidden_layers: 1
  fm_hidden_dim: 192
  embedding_dropout: 0.35621194418689556
  early_stopping_patience: 25
  scheduler_type: none
