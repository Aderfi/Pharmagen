{
  "model": "Phenotype_Effect_Outcome_Kv",
  "optimization_type": "multi_objective",
  "n_trials": 40,
  "completed_trials": 40,
  "best_trial": {
    "number": 4,
    "values": [
      -0.9295412188484555,
      -0.4766708252608879
    ],
    "params": {
      "embedding_dim": 512,
      "n_layers": 2,
      "hidden_dim": 3968,
      "dropout_rate": 0.5613000284153746,
      "weight_decay": 0.00029352732386852306,
      "learning_rate": 0.000439654554875526,
      "batch_size": 64,
      "\n            # === TRANSFORMER ATTENTION (based on your attention_layer) ===\n            \"attention_dim_feedforward\": [\"int\", 512, 4096, 256],  # For transformer feedforward\n            \"attention_dropout\": (0.1, 0.5),\n            \"num_attention_layers\": [1, 2, 3, 4],  # Stack multiple transformer layers\n            \n            # === FOCAL LOSS (you're using it for effect_type) ===\n            \"focal_gamma\": (1.0, 5.0),  # Currently hardcoded to 2.0\n            \"focal_alpha_weight\": (0.5, 3.0),  # Multiplier for class weights\n            \"label_smoothing\": (0.05, 0.3),  # Currently hardcoded to 0.15\n            \n            # === OPTIMIZER VARIANTS ===\n            \"optimizer_type\": [\"adamw\", \"adam\", \"sgd\", \"rmsprop\"],\n            \"adam_beta1\": (0.8, 0.95),\n            \"adam_beta2\": (0.95, 0.999),\n            \"sgd_momentum\": (0.85, 0.99),\n            \n            # === SCHEDULER ===\n            \"scheduler_type\": [\"plateau\", \"cosine\", \"step\", \"exponential\", \"none\"],\n            \"scheduler_factor\": (0.1, 0.8),  # For ReduceLROnPlateau\n            \"scheduler_patience\": [\"int\", 3, 15, 1],\n            \n            # === TASK WEIGHTING ===\n            \"uncertainty_weighting\": [True, False],  # Toggle your uncertainty weighting\n            \"manual_task_weights\": [True, False],  # Use CLINICAL_PRIORITIES vs learned weights\n            \n            # === ADVANCED ARCHITECTURE ===\n            \"use_batch_norm\": [True, False],\n            \"use_layer_norm\": [True, False], \n            \"activation_function\": [\"gelu\", \"relu\", \"swish\", \"mish\"],  # You're using GELU\n            \"gradient_clip_norm\": (0.5, 5.0),\n            \n            # === FM BRANCH ENHANCEMENTS ===\n            \"fm_dropout\": (0.1, 0.5),  # Separate dropout for FM interactions\n            \"fm_hidden_layers\": [0, 1, 2],  # Add layers after FM interactions\n            \"fm_hidden_dim\": [\"int\", 64, 512, 32],\n            \n            # === EMBEDDING VARIATIONS ===\n            \"embedding_dropout\": (0.1, 0.3),\n            \"drug_embedding_dim\": [\"int\", 64, 1024, 32],  # Separate embedding dims\n            \"gene_embedding_dim\": [\"int\", 64, 1024, 32],\n            \"allele_embedding_dim\": [\"int\", 32, 512, 16],\n            \"genalle_embedding_dim\": [\"int\", 64, 1024, 32],\n            \n            # === TRAINING DYNAMICS ===\n            \"warmup_epochs\": [\"int\", 0, 20, 1],\n            \"early_stopping_patience\": [\"int\", 15, 50, 5],\n            \"validation_frequency\": [\"int\", 1, 5, 1],  # Validate every N epochs\n        ": null,
      "attention_dim_feedforward": 2048,
      "attention_dropout": 0.25678901053743164,
      "num_attention_layers": 1,
      "focal_gamma": 2.6657529089835705,
      "focal_alpha_weight": 1.691898591324244,
      "label_smoothing": 0.021285972400163142,
      "optimizer_type": "rmsprop",
      "manual_task_weights": true,
      "use_batch_norm": false,
      "use_layer_norm": true,
      "activation_function": "swish",
      "gradient_clip_norm": 1.9544926603756845,
      "fm_dropout": 0.460209483756725,
      "fm_hidden_layers": 1,
      "fm_hidden_dim": 320,
      "embedding_dropout": 0.15522107054152176,
      "early_stopping_patience": 35,
      "scheduler_type": "plateau",
      "adam_beta1": 0.9424628448704079,
      "adam_beta2": 0.9951479086904819,
      "scheduler_factor": 0.4819244095785148,
      "scheduler_patience": 4
    },
    "user_attrs": {
      "all_accuracies": [
        0.8747467071935157,
        0.6656534954407295,
        0.45782674772036475
      ],
      "avg_accuracy": 0.6660756501182034,
      "critical_task_f1": 0.4766708252608879,
      "critical_task_name": "effect_type",
      "effect_direction_f1_macro": 0.5268460356761328,
      "effect_direction_f1_weighted": 0.6531754012947374,
      "effect_direction_precision_macro": 0.5772653130197355,
      "effect_direction_recall_macro": 0.5028837752695875,
      "effect_type_f1_macro": 0.3552934824225418,
      "effect_type_f1_weighted": 0.4766708252608879,
      "effect_type_precision_macro": 0.3713384271967396,
      "effect_type_recall_macro": 0.3536649903936069,
      "full_params": {
        "embedding_dim": 512,
        "n_layers": 2,
        "hidden_dim": 3968,
        "dropout_rate": 0.5613000284153746,
        "weight_decay": 0.00029352732386852306,
        "learning_rate": 0.000439654554875526,
        "batch_size": 64,
        "\n            # === TRANSFORMER ATTENTION (based on your attention_layer) ===\n            \"attention_dim_feedforward\": [\"int\", 512, 4096, 256],  # For transformer feedforward\n            \"attention_dropout\": (0.1, 0.5),\n            \"num_attention_layers\": [1, 2, 3, 4],  # Stack multiple transformer layers\n            \n            # === FOCAL LOSS (you're using it for effect_type) ===\n            \"focal_gamma\": (1.0, 5.0),  # Currently hardcoded to 2.0\n            \"focal_alpha_weight\": (0.5, 3.0),  # Multiplier for class weights\n            \"label_smoothing\": (0.05, 0.3),  # Currently hardcoded to 0.15\n            \n            # === OPTIMIZER VARIANTS ===\n            \"optimizer_type\": [\"adamw\", \"adam\", \"sgd\", \"rmsprop\"],\n            \"adam_beta1\": (0.8, 0.95),\n            \"adam_beta2\": (0.95, 0.999),\n            \"sgd_momentum\": (0.85, 0.99),\n            \n            # === SCHEDULER ===\n            \"scheduler_type\": [\"plateau\", \"cosine\", \"step\", \"exponential\", \"none\"],\n            \"scheduler_factor\": (0.1, 0.8),  # For ReduceLROnPlateau\n            \"scheduler_patience\": [\"int\", 3, 15, 1],\n            \n            # === TASK WEIGHTING ===\n            \"uncertainty_weighting\": [True, False],  # Toggle your uncertainty weighting\n            \"manual_task_weights\": [True, False],  # Use CLINICAL_PRIORITIES vs learned weights\n            \n            # === ADVANCED ARCHITECTURE ===\n            \"use_batch_norm\": [True, False],\n            \"use_layer_norm\": [True, False], \n            \"activation_function\": [\"gelu\", \"relu\", \"swish\", \"mish\"],  # You're using GELU\n            \"gradient_clip_norm\": (0.5, 5.0),\n            \n            # === FM BRANCH ENHANCEMENTS ===\n            \"fm_dropout\": (0.1, 0.5),  # Separate dropout for FM interactions\n            \"fm_hidden_layers\": [0, 1, 2],  # Add layers after FM interactions\n            \"fm_hidden_dim\": [\"int\", 64, 512, 32],\n            \n            # === EMBEDDING VARIATIONS ===\n            \"embedding_dropout\": (0.1, 0.3),\n            \"drug_embedding_dim\": [\"int\", 64, 1024, 32],  # Separate embedding dims\n            \"gene_embedding_dim\": [\"int\", 64, 1024, 32],\n            \"allele_embedding_dim\": [\"int\", 32, 512, 16],\n            \"genalle_embedding_dim\": [\"int\", 64, 1024, 32],\n            \n            # === TRAINING DYNAMICS ===\n            \"warmup_epochs\": [\"int\", 0, 20, 1],\n            \"early_stopping_patience\": [\"int\", 15, 50, 5],\n            \"validation_frequency\": [\"int\", 1, 5, 1],  # Validate every N epochs\n        ": null,
        "attention_dim_feedforward": 2048,
        "attention_dropout": 0.25678901053743164,
        "num_attention_layers": 1,
        "focal_gamma": 2.6657529089835705,
        "focal_alpha_weight": 1.691898591324244,
        "label_smoothing": 0.021285972400163142,
        "optimizer_type": "rmsprop",
        "manual_task_weights": true,
        "use_batch_norm": false,
        "use_layer_norm": true,
        "activation_function": "swish",
        "gradient_clip_norm": 1.9544926603756845,
        "fm_dropout": 0.460209483756725,
        "fm_hidden_layers": 1,
        "fm_hidden_dim": 320,
        "embedding_dropout": 0.15522107054152176,
        "early_stopping_patience": 35,
        "scheduler_type": "plateau",
        "adam_beta1": 0.9424628448704079,
        "adam_beta2": 0.9951479086904819,
        "scheduler_factor": 0.4819244095785148,
        "scheduler_patience": 4
      },
      "normalized_loss": -0.18315455838346084,
      "phenotype_outcome_f1_macro": 0.5613889220059238,
      "phenotype_outcome_f1_samples": 0.6954787234042553,
      "phenotype_outcome_precision_samples": 0.701177811550152,
      "phenotype_outcome_recall_samples": 0.6997213779128673,
      "seed": 711
    }
  },
  "clinical_priorities": {
    "effect_type": 0.6,
    "phenotype_outcome": 0.25,
    "effect_direction": 0.15
  },
  "pareto_front": [
    {
      "number": 4,
      "values": [
        -0.9295412188484555,
        -0.4766708252608879
      ],
      "params": {
        "embedding_dim": 512,
        "n_layers": 2,
        "hidden_dim": 3968,
        "dropout_rate": 0.5613000284153746,
        "weight_decay": 0.00029352732386852306,
        "learning_rate": 0.000439654554875526,
        "batch_size": 64,
        "\n            # === TRANSFORMER ATTENTION (based on your attention_layer) ===\n            \"attention_dim_feedforward\": [\"int\", 512, 4096, 256],  # For transformer feedforward\n            \"attention_dropout\": (0.1, 0.5),\n            \"num_attention_layers\": [1, 2, 3, 4],  # Stack multiple transformer layers\n            \n            # === FOCAL LOSS (you're using it for effect_type) ===\n            \"focal_gamma\": (1.0, 5.0),  # Currently hardcoded to 2.0\n            \"focal_alpha_weight\": (0.5, 3.0),  # Multiplier for class weights\n            \"label_smoothing\": (0.05, 0.3),  # Currently hardcoded to 0.15\n            \n            # === OPTIMIZER VARIANTS ===\n            \"optimizer_type\": [\"adamw\", \"adam\", \"sgd\", \"rmsprop\"],\n            \"adam_beta1\": (0.8, 0.95),\n            \"adam_beta2\": (0.95, 0.999),\n            \"sgd_momentum\": (0.85, 0.99),\n            \n            # === SCHEDULER ===\n            \"scheduler_type\": [\"plateau\", \"cosine\", \"step\", \"exponential\", \"none\"],\n            \"scheduler_factor\": (0.1, 0.8),  # For ReduceLROnPlateau\n            \"scheduler_patience\": [\"int\", 3, 15, 1],\n            \n            # === TASK WEIGHTING ===\n            \"uncertainty_weighting\": [True, False],  # Toggle your uncertainty weighting\n            \"manual_task_weights\": [True, False],  # Use CLINICAL_PRIORITIES vs learned weights\n            \n            # === ADVANCED ARCHITECTURE ===\n            \"use_batch_norm\": [True, False],\n            \"use_layer_norm\": [True, False], \n            \"activation_function\": [\"gelu\", \"relu\", \"swish\", \"mish\"],  # You're using GELU\n            \"gradient_clip_norm\": (0.5, 5.0),\n            \n            # === FM BRANCH ENHANCEMENTS ===\n            \"fm_dropout\": (0.1, 0.5),  # Separate dropout for FM interactions\n            \"fm_hidden_layers\": [0, 1, 2],  # Add layers after FM interactions\n            \"fm_hidden_dim\": [\"int\", 64, 512, 32],\n            \n            # === EMBEDDING VARIATIONS ===\n            \"embedding_dropout\": (0.1, 0.3),\n            \"drug_embedding_dim\": [\"int\", 64, 1024, 32],  # Separate embedding dims\n            \"gene_embedding_dim\": [\"int\", 64, 1024, 32],\n            \"allele_embedding_dim\": [\"int\", 32, 512, 16],\n            \"genalle_embedding_dim\": [\"int\", 64, 1024, 32],\n            \n            # === TRAINING DYNAMICS ===\n            \"warmup_epochs\": [\"int\", 0, 20, 1],\n            \"early_stopping_patience\": [\"int\", 15, 50, 5],\n            \"validation_frequency\": [\"int\", 1, 5, 1],  # Validate every N epochs\n        ": null,
        "attention_dim_feedforward": 2048,
        "attention_dropout": 0.25678901053743164,
        "num_attention_layers": 1,
        "focal_gamma": 2.6657529089835705,
        "focal_alpha_weight": 1.691898591324244,
        "label_smoothing": 0.021285972400163142,
        "optimizer_type": "rmsprop",
        "manual_task_weights": true,
        "use_batch_norm": false,
        "use_layer_norm": true,
        "activation_function": "swish",
        "gradient_clip_norm": 1.9544926603756845,
        "fm_dropout": 0.460209483756725,
        "fm_hidden_layers": 1,
        "fm_hidden_dim": 320,
        "embedding_dropout": 0.15522107054152176,
        "early_stopping_patience": 35,
        "scheduler_type": "plateau",
        "adam_beta1": 0.9424628448704079,
        "adam_beta2": 0.9951479086904819,
        "scheduler_factor": 0.4819244095785148,
        "scheduler_patience": 4
      },
      "critical_f1": 0.4766708252608879
    },
    {
      "number": 19,
      "values": [
        -1.5744303095908392,
        -0.4568918697147221
      ],
      "params": {
        "embedding_dim": 512,
        "n_layers": 7,
        "hidden_dim": 3200,
        "dropout_rate": 0.5166192773604796,
        "weight_decay": 4.377289565849319e-06,
        "learning_rate": 0.00021466257328365182,
        "batch_size": 64,
        "\n            # === TRANSFORMER ATTENTION (based on your attention_layer) ===\n            \"attention_dim_feedforward\": [\"int\", 512, 4096, 256],  # For transformer feedforward\n            \"attention_dropout\": (0.1, 0.5),\n            \"num_attention_layers\": [1, 2, 3, 4],  # Stack multiple transformer layers\n            \n            # === FOCAL LOSS (you're using it for effect_type) ===\n            \"focal_gamma\": (1.0, 5.0),  # Currently hardcoded to 2.0\n            \"focal_alpha_weight\": (0.5, 3.0),  # Multiplier for class weights\n            \"label_smoothing\": (0.05, 0.3),  # Currently hardcoded to 0.15\n            \n            # === OPTIMIZER VARIANTS ===\n            \"optimizer_type\": [\"adamw\", \"adam\", \"sgd\", \"rmsprop\"],\n            \"adam_beta1\": (0.8, 0.95),\n            \"adam_beta2\": (0.95, 0.999),\n            \"sgd_momentum\": (0.85, 0.99),\n            \n            # === SCHEDULER ===\n            \"scheduler_type\": [\"plateau\", \"cosine\", \"step\", \"exponential\", \"none\"],\n            \"scheduler_factor\": (0.1, 0.8),  # For ReduceLROnPlateau\n            \"scheduler_patience\": [\"int\", 3, 15, 1],\n            \n            # === TASK WEIGHTING ===\n            \"uncertainty_weighting\": [True, False],  # Toggle your uncertainty weighting\n            \"manual_task_weights\": [True, False],  # Use CLINICAL_PRIORITIES vs learned weights\n            \n            # === ADVANCED ARCHITECTURE ===\n            \"use_batch_norm\": [True, False],\n            \"use_layer_norm\": [True, False], \n            \"activation_function\": [\"gelu\", \"relu\", \"swish\", \"mish\"],  # You're using GELU\n            \"gradient_clip_norm\": (0.5, 5.0),\n            \n            # === FM BRANCH ENHANCEMENTS ===\n            \"fm_dropout\": (0.1, 0.5),  # Separate dropout for FM interactions\n            \"fm_hidden_layers\": [0, 1, 2],  # Add layers after FM interactions\n            \"fm_hidden_dim\": [\"int\", 64, 512, 32],\n            \n            # === EMBEDDING VARIATIONS ===\n            \"embedding_dropout\": (0.1, 0.3),\n            \"drug_embedding_dim\": [\"int\", 64, 1024, 32],  # Separate embedding dims\n            \"gene_embedding_dim\": [\"int\", 64, 1024, 32],\n            \"allele_embedding_dim\": [\"int\", 32, 512, 16],\n            \"genalle_embedding_dim\": [\"int\", 64, 1024, 32],\n            \n            # === TRAINING DYNAMICS ===\n            \"warmup_epochs\": [\"int\", 0, 20, 1],\n            \"early_stopping_patience\": [\"int\", 15, 50, 5],\n            \"validation_frequency\": [\"int\", 1, 5, 1],  # Validate every N epochs\n        ": null,
        "attention_dim_feedforward": 1280,
        "attention_dropout": 0.18494617185420625,
        "num_attention_layers": 1,
        "focal_gamma": 1.8842693450502406,
        "focal_alpha_weight": 0.6072485746809503,
        "label_smoothing": 0.12323354734436553,
        "optimizer_type": "adam",
        "manual_task_weights": true,
        "use_batch_norm": false,
        "use_layer_norm": true,
        "activation_function": "relu",
        "gradient_clip_norm": 0.8926213526039128,
        "fm_dropout": 0.31510917370669805,
        "fm_hidden_layers": 1,
        "fm_hidden_dim": 256,
        "embedding_dropout": 0.32480972663849234,
        "early_stopping_patience": 20,
        "scheduler_type": "cosine",
        "adam_beta1": 0.9341796260639474,
        "adam_beta2": 0.9770111074519999,
        "scheduler_factor": 0.24670762641058824,
        "scheduler_patience": 5
      },
      "critical_f1": 0.4568918697147221
    },
    {
      "number": 21,
      "values": [
        -1.9477407137552898,
        -0.4551871764829042
      ],
      "params": {
        "embedding_dim": 512,
        "n_layers": 6,
        "hidden_dim": 2432,
        "dropout_rate": 0.5104572945720455,
        "weight_decay": 0.009261284466214022,
        "learning_rate": 0.00016686539019106342,
        "batch_size": 64,
        "\n            # === TRANSFORMER ATTENTION (based on your attention_layer) ===\n            \"attention_dim_feedforward\": [\"int\", 512, 4096, 256],  # For transformer feedforward\n            \"attention_dropout\": (0.1, 0.5),\n            \"num_attention_layers\": [1, 2, 3, 4],  # Stack multiple transformer layers\n            \n            # === FOCAL LOSS (you're using it for effect_type) ===\n            \"focal_gamma\": (1.0, 5.0),  # Currently hardcoded to 2.0\n            \"focal_alpha_weight\": (0.5, 3.0),  # Multiplier for class weights\n            \"label_smoothing\": (0.05, 0.3),  # Currently hardcoded to 0.15\n            \n            # === OPTIMIZER VARIANTS ===\n            \"optimizer_type\": [\"adamw\", \"adam\", \"sgd\", \"rmsprop\"],\n            \"adam_beta1\": (0.8, 0.95),\n            \"adam_beta2\": (0.95, 0.999),\n            \"sgd_momentum\": (0.85, 0.99),\n            \n            # === SCHEDULER ===\n            \"scheduler_type\": [\"plateau\", \"cosine\", \"step\", \"exponential\", \"none\"],\n            \"scheduler_factor\": (0.1, 0.8),  # For ReduceLROnPlateau\n            \"scheduler_patience\": [\"int\", 3, 15, 1],\n            \n            # === TASK WEIGHTING ===\n            \"uncertainty_weighting\": [True, False],  # Toggle your uncertainty weighting\n            \"manual_task_weights\": [True, False],  # Use CLINICAL_PRIORITIES vs learned weights\n            \n            # === ADVANCED ARCHITECTURE ===\n            \"use_batch_norm\": [True, False],\n            \"use_layer_norm\": [True, False], \n            \"activation_function\": [\"gelu\", \"relu\", \"swish\", \"mish\"],  # You're using GELU\n            \"gradient_clip_norm\": (0.5, 5.0),\n            \n            # === FM BRANCH ENHANCEMENTS ===\n            \"fm_dropout\": (0.1, 0.5),  # Separate dropout for FM interactions\n            \"fm_hidden_layers\": [0, 1, 2],  # Add layers after FM interactions\n            \"fm_hidden_dim\": [\"int\", 64, 512, 32],\n            \n            # === EMBEDDING VARIATIONS ===\n            \"embedding_dropout\": (0.1, 0.3),\n            \"drug_embedding_dim\": [\"int\", 64, 1024, 32],  # Separate embedding dims\n            \"gene_embedding_dim\": [\"int\", 64, 1024, 32],\n            \"allele_embedding_dim\": [\"int\", 32, 512, 16],\n            \"genalle_embedding_dim\": [\"int\", 64, 1024, 32],\n            \n            # === TRAINING DYNAMICS ===\n            \"warmup_epochs\": [\"int\", 0, 20, 1],\n            \"early_stopping_patience\": [\"int\", 15, 50, 5],\n            \"validation_frequency\": [\"int\", 1, 5, 1],  # Validate every N epochs\n        ": null,
        "attention_dim_feedforward": 2048,
        "attention_dropout": 0.1341612430477401,
        "num_attention_layers": 1,
        "focal_gamma": 2.942952147882995,
        "focal_alpha_weight": 0.4269435183821836,
        "label_smoothing": 0.12207349170627256,
        "optimizer_type": "adamw",
        "manual_task_weights": true,
        "use_batch_norm": false,
        "use_layer_norm": true,
        "activation_function": "swish",
        "gradient_clip_norm": 1.3024984235114774,
        "fm_dropout": 0.18555556199900858,
        "fm_hidden_layers": 0,
        "fm_hidden_dim": 512,
        "embedding_dropout": 0.25669538952814885,
        "early_stopping_patience": 25,
        "scheduler_type": "none",
        "adam_beta1": 0.9485501616396188,
        "adam_beta2": 0.9843388349023521,
        "scheduler_factor": 0.42131113041551627,
        "scheduler_patience": 7
      },
      "critical_f1": 0.4551871764829042
    },
    {
      "number": 38,
      "values": [
        -3.257747871535165,
        -0.4440707949461575
      ],
      "params": {
        "embedding_dim": 384,
        "n_layers": 5,
        "hidden_dim": 3840,
        "dropout_rate": 0.5730727676920662,
        "weight_decay": 0.00039773826324692477,
        "learning_rate": 0.000605212882291598,
        "batch_size": 64,
        "\n            # === TRANSFORMER ATTENTION (based on your attention_layer) ===\n            \"attention_dim_feedforward\": [\"int\", 512, 4096, 256],  # For transformer feedforward\n            \"attention_dropout\": (0.1, 0.5),\n            \"num_attention_layers\": [1, 2, 3, 4],  # Stack multiple transformer layers\n            \n            # === FOCAL LOSS (you're using it for effect_type) ===\n            \"focal_gamma\": (1.0, 5.0),  # Currently hardcoded to 2.0\n            \"focal_alpha_weight\": (0.5, 3.0),  # Multiplier for class weights\n            \"label_smoothing\": (0.05, 0.3),  # Currently hardcoded to 0.15\n            \n            # === OPTIMIZER VARIANTS ===\n            \"optimizer_type\": [\"adamw\", \"adam\", \"sgd\", \"rmsprop\"],\n            \"adam_beta1\": (0.8, 0.95),\n            \"adam_beta2\": (0.95, 0.999),\n            \"sgd_momentum\": (0.85, 0.99),\n            \n            # === SCHEDULER ===\n            \"scheduler_type\": [\"plateau\", \"cosine\", \"step\", \"exponential\", \"none\"],\n            \"scheduler_factor\": (0.1, 0.8),  # For ReduceLROnPlateau\n            \"scheduler_patience\": [\"int\", 3, 15, 1],\n            \n            # === TASK WEIGHTING ===\n            \"uncertainty_weighting\": [True, False],  # Toggle your uncertainty weighting\n            \"manual_task_weights\": [True, False],  # Use CLINICAL_PRIORITIES vs learned weights\n            \n            # === ADVANCED ARCHITECTURE ===\n            \"use_batch_norm\": [True, False],\n            \"use_layer_norm\": [True, False], \n            \"activation_function\": [\"gelu\", \"relu\", \"swish\", \"mish\"],  # You're using GELU\n            \"gradient_clip_norm\": (0.5, 5.0),\n            \n            # === FM BRANCH ENHANCEMENTS ===\n            \"fm_dropout\": (0.1, 0.5),  # Separate dropout for FM interactions\n            \"fm_hidden_layers\": [0, 1, 2],  # Add layers after FM interactions\n            \"fm_hidden_dim\": [\"int\", 64, 512, 32],\n            \n            # === EMBEDDING VARIATIONS ===\n            \"embedding_dropout\": (0.1, 0.3),\n            \"drug_embedding_dim\": [\"int\", 64, 1024, 32],  # Separate embedding dims\n            \"gene_embedding_dim\": [\"int\", 64, 1024, 32],\n            \"allele_embedding_dim\": [\"int\", 32, 512, 16],\n            \"genalle_embedding_dim\": [\"int\", 64, 1024, 32],\n            \n            # === TRAINING DYNAMICS ===\n            \"warmup_epochs\": [\"int\", 0, 20, 1],\n            \"early_stopping_patience\": [\"int\", 15, 50, 5],\n            \"validation_frequency\": [\"int\", 1, 5, 1],  # Validate every N epochs\n        ": null,
        "attention_dim_feedforward": 1024,
        "attention_dropout": 0.1021892777422384,
        "num_attention_layers": 1,
        "focal_gamma": 2.5717828292105254,
        "focal_alpha_weight": 0.4408327356158774,
        "label_smoothing": 0.008576573250239979,
        "optimizer_type": "rmsprop",
        "manual_task_weights": true,
        "use_batch_norm": true,
        "use_layer_norm": true,
        "activation_function": "gelu",
        "gradient_clip_norm": 3.309626121447729,
        "fm_dropout": 0.36003745089432343,
        "fm_hidden_layers": 1,
        "fm_hidden_dim": 256,
        "embedding_dropout": 0.24323467017432807,
        "early_stopping_patience": 20,
        "scheduler_type": "none",
        "adam_beta1": 0.8502799193800216,
        "adam_beta2": 0.9976945068558812,
        "scheduler_factor": 0.308478243545683,
        "scheduler_patience": 10
      },
      "critical_f1": 0.4440707949461575
    },
    {
      "number": 39,
      "values": [
        -3.527317098208836,
        -0.4430012539024594
      ],
      "params": {
        "embedding_dim": 128,
        "n_layers": 5,
        "hidden_dim": 3072,
        "dropout_rate": 0.39068520160372305,
        "weight_decay": 5.0317738624675386e-05,
        "learning_rate": 0.0005730468431792992,
        "batch_size": 64,
        "\n            # === TRANSFORMER ATTENTION (based on your attention_layer) ===\n            \"attention_dim_feedforward\": [\"int\", 512, 4096, 256],  # For transformer feedforward\n            \"attention_dropout\": (0.1, 0.5),\n            \"num_attention_layers\": [1, 2, 3, 4],  # Stack multiple transformer layers\n            \n            # === FOCAL LOSS (you're using it for effect_type) ===\n            \"focal_gamma\": (1.0, 5.0),  # Currently hardcoded to 2.0\n            \"focal_alpha_weight\": (0.5, 3.0),  # Multiplier for class weights\n            \"label_smoothing\": (0.05, 0.3),  # Currently hardcoded to 0.15\n            \n            # === OPTIMIZER VARIANTS ===\n            \"optimizer_type\": [\"adamw\", \"adam\", \"sgd\", \"rmsprop\"],\n            \"adam_beta1\": (0.8, 0.95),\n            \"adam_beta2\": (0.95, 0.999),\n            \"sgd_momentum\": (0.85, 0.99),\n            \n            # === SCHEDULER ===\n            \"scheduler_type\": [\"plateau\", \"cosine\", \"step\", \"exponential\", \"none\"],\n            \"scheduler_factor\": (0.1, 0.8),  # For ReduceLROnPlateau\n            \"scheduler_patience\": [\"int\", 3, 15, 1],\n            \n            # === TASK WEIGHTING ===\n            \"uncertainty_weighting\": [True, False],  # Toggle your uncertainty weighting\n            \"manual_task_weights\": [True, False],  # Use CLINICAL_PRIORITIES vs learned weights\n            \n            # === ADVANCED ARCHITECTURE ===\n            \"use_batch_norm\": [True, False],\n            \"use_layer_norm\": [True, False], \n            \"activation_function\": [\"gelu\", \"relu\", \"swish\", \"mish\"],  # You're using GELU\n            \"gradient_clip_norm\": (0.5, 5.0),\n            \n            # === FM BRANCH ENHANCEMENTS ===\n            \"fm_dropout\": (0.1, 0.5),  # Separate dropout for FM interactions\n            \"fm_hidden_layers\": [0, 1, 2],  # Add layers after FM interactions\n            \"fm_hidden_dim\": [\"int\", 64, 512, 32],\n            \n            # === EMBEDDING VARIATIONS ===\n            \"embedding_dropout\": (0.1, 0.3),\n            \"drug_embedding_dim\": [\"int\", 64, 1024, 32],  # Separate embedding dims\n            \"gene_embedding_dim\": [\"int\", 64, 1024, 32],\n            \"allele_embedding_dim\": [\"int\", 32, 512, 16],\n            \"genalle_embedding_dim\": [\"int\", 64, 1024, 32],\n            \n            # === TRAINING DYNAMICS ===\n            \"warmup_epochs\": [\"int\", 0, 20, 1],\n            \"early_stopping_patience\": [\"int\", 15, 50, 5],\n            \"validation_frequency\": [\"int\", 1, 5, 1],  # Validate every N epochs\n        ": null,
        "attention_dim_feedforward": 1024,
        "attention_dropout": 0.16425453485782834,
        "num_attention_layers": 1,
        "focal_gamma": 2.8837712550980745,
        "focal_alpha_weight": 0.3379788567248563,
        "label_smoothing": 0.007241126339333478,
        "optimizer_type": "rmsprop",
        "manual_task_weights": true,
        "use_batch_norm": true,
        "use_layer_norm": true,
        "activation_function": "relu",
        "gradient_clip_norm": 3.5881973750267306,
        "fm_dropout": 0.4495053652272731,
        "fm_hidden_layers": 1,
        "fm_hidden_dim": 192,
        "embedding_dropout": 0.35621194418689556,
        "early_stopping_patience": 25,
        "scheduler_type": "none",
        "adam_beta1": 0.8521022936071257,
        "adam_beta2": 0.9965192513410643,
        "scheduler_factor": 0.364725047084997,
        "scheduler_patience": 8
      },
      "critical_f1": 0.4430012539024594
    }
  ]
}